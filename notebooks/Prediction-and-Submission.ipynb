{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapping-challenge-mask_rcnn-prediction-submission\n",
    "![CrowdAI-Logo](https://github.com/crowdAI/crowdai/raw/master/app/assets/images/misc/crowdai-logo-smile.svg?sanitize=true)\n",
    "\n",
    "This notebook contains the code for making predictions from the model trained in [Training.ipynb](Training.ipynb) (or by using the [released pretrained model](https://www.crowdai.org/challenges/mapping-challenge/dataset_files)) for the [crowdAI Mapping Challenge](https://www.crowdai.org/challenges/mapping-challenge).\n",
    "\n",
    "This code is adapted from the [Mask RCNN]() tensorflow implementation available here : [https://github.com/matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN).\n",
    "\n",
    "First we begin by importing all the necessary dependencies : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "\n",
    "# Download and install the Python COCO tools from https://github.com/waleedka/coco\n",
    "# That's a fork from the original https://github.com/pdollar/coco with a bug\n",
    "# fix for Python 3.\n",
    "# I submitted a pull request https://github.com/cocodataset/cocoapi/pull/50\n",
    "# If the PR is merged then use the original repo.\n",
    "# Note: Edit PythonAPI/Makefile and replace \"python\" with \"python3\".\n",
    "#  \n",
    "# A quick one liner to install the library \n",
    "# !pip install git+https://github.com/waleedka/coco.git#subdirectory=PythonAPI\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "import coco #a slightly modified version\n",
    "\n",
    "from mrcnn.evaluate import build_coco_results, evaluate_coco\n",
    "from mrcnn.dataset import MappingChallengeDataset\n",
    "from mrcnn import visualize\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "import glob\n",
    "import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset location \n",
    "Now we expect that you have downloaded all the files in the datasets section and untar-ed them to have the following structure :\n",
    "```\n",
    "├── data\n",
    "|   ├── pretrained_weights.h5 (already included in this repository)\n",
    "│   ├── test\n",
    "│   │   └── images/\n",
    "│   │   └── annotation.json\n",
    "│   ├── train\n",
    "│   │   └── images/\n",
    "│   │   └── annotation.json\n",
    "│   └── val\n",
    "│       └── images/\n",
    "│       └── annotation.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macintosh/Arista/work/pena/giz/giz/notebooks/logs\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL_PATH = os.path.join(ROOT_DIR,\"data/\" \"pretrained_weights.h5\")\n",
    "# PRETRAINED_MODEL_PATH = os.path.join(ROOT_DIR,\"data/\\\" \\\"pretrained_weights.h5\\\"),\n",
    "LOGS_DIRECTORY = os.path.join(ROOT_DIR, \"logs\")\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"data\", \"test\", \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantitate Inference Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     5\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 5\n",
      "IMAGE_MAX_DIM                  320\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  320\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [320 320   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           crowdai-mapping-challenge\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n",
      "<__main__.InferenceConfig object at 0x29031fca0>\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 5\n",
    "    NUM_CLASSES = 1 + 1  # 1 Background + 1 Building\n",
    "    IMAGE_MAX_DIM=320\n",
    "    IMAGE_MIN_DIM=320\n",
    "    NAME = \"crowdai-mapping-challenge\"\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"mrcnn_bbox\" (type Reshape).\n\nTried to convert 'shape' to a tensor and failed. Error: None values not supported.\n\nCall arguments received by layer \"mrcnn_bbox\" (type Reshape):\n  • inputs=tf.Tensor(shape=(1, None, 8), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m modelgo\u001b[39m.\u001b[39;49mMaskRCNN(mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minference\u001b[39;49m\u001b[39m\"\u001b[39;49m, model_dir\u001b[39m=\u001b[39;49mMODEL_DIR, config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m      4\u001b[0m model_path \u001b[39m=\u001b[39m PRETRAINED_MODEL_PATH\n\u001b[1;32m      6\u001b[0m \u001b[39m# or if you want to use the latest trained model, you can use : \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# model_path = model.find_last()[1]\u001b[39;00m\n",
      "File \u001b[0;32m~/Arista/work/pena/giz/giz/notebooks/mrcnn/model.py:1840\u001b[0m, in \u001b[0;36mMaskRCNN.__init__\u001b[0;34m(self, mode, config, model_dir)\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_dir \u001b[39m=\u001b[39m model_dir\n\u001b[1;32m   1839\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_log_dir()\n\u001b[0;32m-> 1840\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeras_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild(mode\u001b[39m=\u001b[39;49mmode, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/Arista/work/pena/giz/giz/notebooks/mrcnn/model.py:2033\u001b[0m, in \u001b[0;36mMaskRCNN.build\u001b[0;34m(self, mode, config)\u001b[0m\n\u001b[1;32m   2028\u001b[0m     model \u001b[39m=\u001b[39m KM\u001b[39m.\u001b[39mModel(inputs, outputs, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmask_rcnn\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2029\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2030\u001b[0m     \u001b[39m# Network Heads\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m     \u001b[39m# Proposal classifier and BBox regressor heads\u001b[39;00m\n\u001b[1;32m   2032\u001b[0m     mrcnn_class_logits, mrcnn_class, mrcnn_bbox \u001b[39m=\u001b[39m\\\n\u001b[0;32m-> 2033\u001b[0m         fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,\n\u001b[1;32m   2034\u001b[0m                              config\u001b[39m.\u001b[39;49mPOOL_SIZE, config\u001b[39m.\u001b[39;49mNUM_CLASSES,\n\u001b[1;32m   2035\u001b[0m                              train_bn\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mTRAIN_BN)\n\u001b[1;32m   2037\u001b[0m     \u001b[39m# Detections\u001b[39;00m\n\u001b[1;32m   2038\u001b[0m     \u001b[39m# output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in \u001b[39;00m\n\u001b[1;32m   2039\u001b[0m     \u001b[39m# normalized coordinates\u001b[39;00m\n\u001b[1;32m   2040\u001b[0m     detections \u001b[39m=\u001b[39m DetectionLayer(config, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmrcnn_detection\u001b[39m\u001b[39m\"\u001b[39m)(\n\u001b[1;32m   2041\u001b[0m         [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])\n",
      "File \u001b[0;32m~/Arista/work/pena/giz/giz/notebooks/mrcnn/model.py:954\u001b[0m, in \u001b[0;36mfpn_classifier_graph\u001b[0;34m(rois, feature_maps, image_meta, pool_size, num_classes, train_bn)\u001b[0m\n\u001b[1;32m    952\u001b[0m s \u001b[39m=\u001b[39m K\u001b[39m.\u001b[39mint_shape(x)\n\u001b[1;32m    953\u001b[0m \u001b[39m# mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\"mrcnn_bbox\")(x)\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m mrcnn_bbox \u001b[39m=\u001b[39m KL\u001b[39m.\u001b[39;49mReshape((s[\u001b[39m1\u001b[39;49m], num_classes, \u001b[39m4\u001b[39;49m), name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmrcnn_bbox\u001b[39;49m\u001b[39m\"\u001b[39;49m)(x)\n\u001b[1;32m    956\u001b[0m \u001b[39mreturn\u001b[39;00m mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n",
      "File \u001b[0;32m~/mambaforge/envs/giz-environment/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/mambaforge/envs/giz-environment/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:573\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    570\u001b[0m   observed \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    571\u001b[0m       values, as_ref\u001b[39m=\u001b[39minput_arg\u001b[39m.\u001b[39mis_ref)\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mname\n\u001b[1;32m    572\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 573\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTried to convert \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00minput_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to a tensor and failed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: \u001b[39m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m prefix \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m Op has type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m that does not match\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    577\u001b[0m           (input_name, op_type_name, observed))\n\u001b[1;32m    578\u001b[0m \u001b[39mif\u001b[39;00m input_arg\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m types_pb2\u001b[39m.\u001b[39mDT_INVALID:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"mrcnn_bbox\" (type Reshape).\n\nTried to convert 'shape' to a tensor and failed. Error: None values not supported.\n\nCall arguments received by layer \"mrcnn_bbox\" (type Reshape):\n  • inputs=tf.Tensor(shape=(1, None, 8), dtype=float32)"
     ]
    }
   ],
   "source": [
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "\n",
    "model_path = PRETRAINED_MODEL_PATH\n",
    "\n",
    "# or if you want to use the latest trained model, you can use : \n",
    "# model_path = model.find_last()[1]\n",
    "\n",
    "model.load_weights(model_path, by_name=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Prediction on a single Image (and visualize results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['BG', 'building'] # In our case, we have 1 class for the background, and 1 class for building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "\n",
    "# file_names = os.listdir(IMAGE_DIR)\n",
    "# file_names = os.path.abspath(IMAGE_DIR)\n",
    "random_image = skimage.io.imread(os.path.join(IMAGE_DIR, \"000000000005.jpg\"))\n",
    "\n",
    "predictions = model.detect([random_image]*config.BATCH_SIZE, verbose=1) # We are replicating the same image to fill up the batch_size\n",
    "\n",
    "p = predictions[0]\n",
    "visualize.display_instances(random_image, p['rois'], p['masks'], p['class_ids'], \n",
    "                            class_names, p['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "file_name = \"000000000005.jpg\"\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(os.path.join(IMAGE_DIR, file_name))\n",
    "\n",
    "# Detect objects in the image\n",
    "predictions = model.detect([image]*config.BATCH_SIZE, verbose=1)\n",
    "p = predictions[0]\n",
    "\n",
    "# Get the class IDs of the detected objects\n",
    "class_ids = p['class_ids']\n",
    "\n",
    "# Get the indices of the detected buildings\n",
    "building_indices = [i for i, class_id in enumerate(class_ids) if class_id == class_names.index(\"building\")]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Draw bounding box and mask overlay on original image\n",
    "for i in building_indices:\n",
    "    # Get the bounding box coordinates of the building\n",
    "    y1, x1, y2, x2 = p['rois'][i]\n",
    "\n",
    "    # Draw the bounding box on the image\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Get the mask for the building\n",
    "    mask = p['masks'][:,:,i]\n",
    "\n",
    "    # Draw the mask overlay on the image\n",
    "    mask_overlay = (0.3 * image + 0.7 * (mask[..., None] > 0) * [255, 255, 255]).astype('uint8')\n",
    "    mask_overlay[mask == 0] = [0, 0, 0]  # Set the unmasked part to black\n",
    "    image = cv2.addWeighted(image, 0.5, mask_overlay, 0.5, 0)\n",
    "\n",
    "    # Crop the building from the image\n",
    "    building = image[y1:y2, x1:x2]\n",
    "\n",
    "    # Save the building as a separate file\n",
    "    cv2.imwrite(f\"building-5-{i}.jpg\", building)\n",
    "\n",
    "    image2 = cv2.cvtColor(building, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply bitwise thresholding to convert the image to black and white\n",
    "    _, bw_image = cv2.threshold(image2, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    n_black_pix = np.sum(bw_image == 0)\n",
    "    n_white_pix = np.sum(bw_image == 255)\n",
    "    # print('Number of black pixels:', n_black_pix)\n",
    "    # print('Number of white pixels:', n_white_pix)\n",
    "\n",
    "    # print('Rooftop area:', n_white_pix)\n",
    "    # print('Rooftop percentage:', n_white_pix/(n_black_pix+n_white_pix))\n",
    "    # Compute the rooftop area and percentage\n",
    "    rooftop_area = n_white_pix\n",
    "    rooftop_percentage = n_white_pix / (n_black_pix + n_white_pix)\n",
    "\n",
    "    # Add the results to the list\n",
    "    results.append({\n",
    "        \"File Name\": file_name,\n",
    "        \"Building Index\": i,\n",
    "        \"Rooftop Area\": rooftop_area,\n",
    "        \"Rooftop Percentage\": rooftop_percentage\n",
    "    })\n",
    "\n",
    "# Create a pandas dataframe from the results list\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3.937] global loadsave.cpp:244 findDecoder imread_('building-5-0.jpg'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "#Area calculation\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image in grayscale mode\n",
    "image = cv2.imread(\"building-5-0.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Apply bitwise thresholding to convert the image to black and white\n",
    "_, bw_image = cv2.threshold(image, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "n_black_pix = np.sum(bw_image == 0)\n",
    "n_white_pix = np.sum(bw_image == 255)\n",
    "print('Number of black pixels:', n_black_pix)\n",
    "print('Number of white pixels:', n_white_pix)\n",
    "\n",
    "print('Rooftop area:', n_white_pix)\n",
    "print('Rooftop percentage:', n_white_pix/(n_black_pix+n_white_pix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p['rois'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7b093ad68a304ca8b93b885ecdb85f34bb53131d6556fc01bf6caf3d7fec0b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
